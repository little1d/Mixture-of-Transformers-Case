# MoT (Mixture of Transformers) Architecture

## 1. æ€»ä½“æ¶æ„å¯¹æ¯”

### Traditional Transformer vs MoT

```
Traditional Transformer:                    MoT (Mixture of Transformers):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input Sequence    â”‚                     â”‚   Input Sequence    â”‚
â”‚ [T1, T2, I1, I2]    â”‚                     â”‚ [T1, T2, I1, I2]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                           â”‚
          â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Shared Attention  â”‚                     â”‚  Modality Routing   â”‚
â”‚  (same params for   â”‚                     â”‚  T1,T2â†’Text Expert  â”‚
â”‚   all tokens)       â”‚                     â”‚  I1,I2â†’Image Expert â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                           â”‚
          â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Shared FFN        â”‚                     â”‚ ModalityUntiedAttn  â”‚
â”‚  (same params for   â”‚                     â”‚ + ModalitySpecific  â”‚
â”‚   all tokens)       â”‚                     â”‚        FFN          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. MoTè¯¦ç»†æ¶æ„å›¾

```
Input: x = [Text_tokens, Image_tokens]  Shape: [batch, seq_len, dim]
       â”‚
       â”œâ”€ Modality Masks Creation
       â”‚  â”œâ”€ text_mask:  [1,1,0,0,0,0,0,0]
       â”‚  â””â”€ image_mask: [0,0,1,1,1,1,1,1]
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MoT Transformer Block                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ModalityUntiedAttention â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  Step 1: Modality-Specific QKV Projection                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚
â”‚  â”‚  â”‚ Text Tokens  â”‚              â”‚ Image Tokens â”‚           â”‚  â”‚
â”‚  â”‚  â”‚   [T1, T2]   â”‚              â”‚[I1,I2,I3,I4] â”‚           â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚
â”‚  â”‚         â”‚                             â”‚                   â”‚  â”‚
â”‚  â”‚         â–¼                             â–¼                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚
â”‚  â”‚  â”‚ Text Expert  â”‚              â”‚ Image Expert â”‚           â”‚  â”‚
â”‚  â”‚  â”‚   Wq_text    â”‚              â”‚   Wq_image   â”‚           â”‚  â”‚
â”‚  â”‚  â”‚   Wk_text    â”‚              â”‚   Wk_image   â”‚           â”‚  â”‚
â”‚  â”‚  â”‚   Wv_text    â”‚              â”‚   Wv_image   â”‚           â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚
â”‚  â”‚         â”‚                             â”‚                   â”‚  â”‚
â”‚  â”‚         â–¼                             â–¼                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚
â”‚  â”‚  â”‚   Q_text     â”‚              â”‚   Q_image    â”‚           â”‚  â”‚
â”‚  â”‚  â”‚   K_text     â”‚              â”‚   K_image    â”‚           â”‚  â”‚
â”‚  â”‚  â”‚   V_text     â”‚              â”‚   V_image    â”‚           â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚
â”‚  â”‚         â”‚                             â”‚                   â”‚  â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â”‚
â”‚  â”‚                   â”‚                                       â”‚  â”‚
â”‚  â”‚  Step 2: Merge & Global Attention ğŸŒŸ                     â”‚  â”‚
â”‚  â”‚                   â–¼                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚          Global Q, K, V Tensors                    â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  Q = [Q_text_T1, Q_text_T2, Q_img_I1, Q_img_I2,  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚       Q_img_I3, Q_img_I4]                         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  K = [K_text_T1, K_text_T2, K_img_I1, K_img_I2,  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚       K_img_I3, K_img_I4]                         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  V = [V_text_T1, V_text_T2, V_img_I1, V_img_I2,  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚       V_img_I3, V_img_I4]                         â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                   â”‚                                       â”‚  â”‚
â”‚  â”‚                   â–¼                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚             Attention Matrix                        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    Scores = Q @ K.T / âˆš(head_dim)                  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚                                                     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚       T1   T2   I1   I2   I3   I4                  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  T1 â”‚ â—    â—    â—    â—    â—    â— â”‚ â†Cross-modal   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  T2 â”‚ â—    â—    â—    â—    â—    â— â”‚   attention!   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  I1 â”‚ â—    â—    â—    â—    â—    â— â”‚                â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  I2 â”‚ â—    â—    â—    â—    â—    â— â”‚                â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  I3 â”‚ â—    â—    â—    â—    â—    â— â”‚                â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  I4 â”‚ â—    â—    â—    â—    â—    â— â”‚                â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                   â”‚                                       â”‚  â”‚
â”‚  â”‚                   â–¼                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚            Attention Output                         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   AttnOut = Softmax(Scores) @ V                    â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                   â”‚                                       â”‚  â”‚
â”‚  â”‚  Step 3: Modality-Specific Output Projection             â”‚  â”‚
â”‚  â”‚                   â–¼                                       â”‚  â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚         â”‚ Text Tokens â”‚              â”‚Image Tokens â”‚      â”‚  â”‚
â”‚  â”‚         â”‚ [AttnOut_T] â”‚              â”‚[AttnOut_I]  â”‚      â”‚  â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â”‚                â”‚                            â”‚             â”‚  â”‚
â”‚  â”‚                â–¼                            â–¼             â”‚  â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚         â”‚ Text Expert â”‚              â”‚Image Expert â”‚      â”‚  â”‚
â”‚  â”‚         â”‚   Wo_text   â”‚              â”‚  Wo_image   â”‚      â”‚  â”‚
â”‚  â”‚         â”‚ Norm_text   â”‚              â”‚ Norm_image  â”‚      â”‚  â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â”‚                â”‚                            â”‚             â”‚  â”‚
â”‚  â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚  â”‚
â”‚  â”‚                             â”‚                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ModalitySpecificFFN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                             â”‚                             â”‚  â”‚
â”‚  â”‚                             â–¼                             â”‚  â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚         â”‚ Text Tokens â”‚              â”‚Image Tokens â”‚      â”‚  â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â”‚                â”‚                            â”‚             â”‚  â”‚
â”‚  â”‚                â–¼                            â–¼             â”‚  â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚         â”‚ Text Expert â”‚              â”‚Image Expert â”‚      â”‚  â”‚
â”‚  â”‚         â”‚  FFN_text   â”‚              â”‚  FFN_image  â”‚      â”‚  â”‚
â”‚  â”‚         â”‚ Norm_text   â”‚              â”‚ Norm_image  â”‚      â”‚  â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â”‚                â”‚                            â”‚             â”‚  â”‚
â”‚  â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚  â”‚
â”‚  â”‚                             â”‚                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚    Final Output         â”‚
                   â”‚ [T1', T2', I1', I2',    â”‚
                   â”‚  I3', I4']              â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3. Global Attentionçš„å…³é”®ä½œç”¨ ğŸŒŸ

### 3.1 Cross-Modal Information Exchange

```
Before Global Attention:
Text tokens: [T1, T2] (processed by text-specific parameters)
Image tokens: [I1, I2, I3, I4] (processed by image-specific parameters)
â†“
Problem: No interaction between modalities!

After Global Attention:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Attention allows every token to            â”‚
â”‚         attend to every other token:               â”‚
â”‚                                                     â”‚
â”‚  T1 â†â†’ T2: Text-to-text interaction               â”‚
â”‚  T1 â†â†’ I1,I2,I3,I4: Text-to-image interaction     â”‚
â”‚  I1 â†â†’ I2,I3,I4: Image-to-image interaction       â”‚
â”‚  I1 â†â†’ T1,T2: Image-to-text interaction           â”‚
â”‚                                                     â”‚
â”‚  ğŸ¯ This enables cross-modal understanding!        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Attention Matrixè¯¦è§£

```
        Query Tokens
        T1  T2  I1  I2  I3  I4
    T1 â”‚0.8 0.6 0.3 0.2 0.1 0.2â”‚ â† T1 mostly attends to text
K   T2 â”‚0.7 0.9 0.1 0.1 0.0 0.1â”‚ â† T2 focuses on T1,T2
e   I1 â”‚0.3 0.2 0.8 0.7 0.6 0.5â”‚ â† I1 attends to images + some text
y   I2 â”‚0.2 0.1 0.6 0.9 0.8 0.7â”‚ â† I2 focuses on images
    I3 â”‚0.1 0.0 0.5 0.6 0.9 0.8â”‚ â† I3 mostly attends to images
    I4 â”‚0.2 0.1 0.4 0.5 0.7 0.9â”‚ â† I4 focuses on nearby images

Cross-modal connections: T1â†”I1 (0.3), T2â†”I1 (0.2), etc.
```

## 4. MoTçš„æ ¸å¿ƒä¼˜åŠ¿

### 4.1 æ¨¡æ€ä¸“ç”¨åŒ– + è·¨æ¨¡æ€äº¤äº’

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Global    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Text Expert      â”‚  Attention   â”‚   Image Expert     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Specialized  â”‚  â”‚              â”‚  â”‚ Specialized  â”‚  â”‚
â”‚  â”‚ for text     â”‚  â”‚              â”‚  â”‚ for images   â”‚  â”‚
â”‚  â”‚ patterns     â”‚  â”‚              â”‚  â”‚ patterns     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Best of both worlds:
âœ… Specialized processing per modality
âœ… Cross-modal information exchange
```

### 4.2 å‚æ•°æ•ˆç‡å¯¹æ¯”

```
Traditional Transformer:
- Shared parameters for all modalities
- May not capture modality-specific patterns well

MoT:
- 2x parameters (one set per modality)
- But much better performance on multimodal tasks
- Each modality gets optimized representations
```

## 5. ä»£ç ä¸­çš„ä½“ç°

### 5.1 æ¨¡æ€è·¯ç”±

```python
# Step 1: Route tokens to modality experts
for i in range(self.n_modalities):
    mask = modality_masks[i]  # [True, True, False, False] for text
    expert_input = x[mask]    # Extract text tokens
    
    # Apply modality-specific projections
    xq = self.local_experts_wq[i](expert_input)  # Text-specific Wq
    xk = self.local_experts_wk[i](expert_input)  # Text-specific Wk  
    xv = self.local_experts_wv[i](expert_input)  # Text-specific Wv
```

### 5.2 Global Attention

```python
# Step 2: Merge and compute global attention
xq = merge_modalities(expert_outputs_xq, modality_masks, target_shape)
xk = merge_modalities(expert_outputs_xk, modality_masks, target_shape)
xv = merge_modalities(expert_outputs_xv, modality_masks, target_shape)

# Global attention computation - ALL tokens interact!
scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(head_dim)
attn_weights = F.softmax(scores, dim=-1)
attn_output = torch.matmul(attn_weights, xv)
```

### 5.3 æ¨¡æ€ä¸“ç”¨è¾“å‡ºå¤„ç†

```python
# Step 3: Route outputs back to modality experts
for i in range(self.n_modalities):
    mask = modality_masks[i]
    expert_input = attn_output[mask]
    
    # Apply modality-specific output projection and normalization
    expert_output = self.local_experts_wo[i](expert_input)
    expert_output = self.local_experts_attention_norm[i](expert_output)
    final_output[mask] = expert_output
```
